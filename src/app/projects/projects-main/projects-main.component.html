<div class="container">
	<div class="row mb-5">
		<div class="col">
			<h3 class="headline">Project / プロジェクト
				<hr />
			</h3>
		</div>
	</div>
</div>

<div class="container">
	<div id="project" class="project-body"></div>

	<section class="card">
		<div class='row'>
			<div class='col-md-5 mb-1'>
				<img src="images/projects/eh_phf.gif" class='project-image'>
			</div>
			<div class='col-md-7 mb-1'>
				<div class='project-title' id="eh-phf">
					<a routerLink="/projects/eh_phf" routerLinkActive="active"
					class="project-link">投影バーチャルハンドインタフェースに対する疑似触覚フィードバックの応用</a></div>
				<!-- <div class='project-title'>投影バーチャルハンドインタフェースに対する疑似触覚フィードバックの応用</div> -->
				<div class='project-publication'>(IEEE Access 2020, IEEE World Haptics 2019 Poster, Demo)</div>
				<div class='project-abstract mt-3'>
					ユーザの手の動きに連動して動くバーチャルハンドをプロジェクタから投影する投影バーチャルハンドインタフェースは、
					日常生活でのユーザの手の届く範囲を拡張することができます。
					しかし、投影バーチャルハンドが実物体に重なった際、ユーザはその物体に触れている感覚を得られません。
					本プロジェクトでは、投影バーチャルハンドが実物体に触れた際に、バーチャルハンドに視覚効果を付与することで、
					疑似触覚フィードバックによりユーザにその物体の触感を知覚させる手法を提案しました。
				</div>
			</div>
		</div>
	</section>
<div class="mb-5"></div>

<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/farfeel.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/projects/farfeel" routerLinkActive="active"
				  class="project-link">fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic
				  Feedback</a></div>
			<!-- <div class='project-title'>fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic
				Feedback</div> -->
			<div class='project-publication'>(IEEE VR 2019 Demo)</div>
			<div class='project-abstract mt-3'>
				We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local
				user to feel touching distant objects. The system allows the local and remote users to communicate by
				using the projected virtual hand (VH) for the agency of his/her own hands. The necessary haptic
				information is provided to the non-manipulating hand of the local user that does not bother the
				manipulation of the projected VH. We also introduce the possible visual stimulus that could potentially
				provide the sense of the body ownership over the projected VH.
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>


<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/responsive_eh.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/projects/responsive_eh" routerLinkActive="active"
				class="project-link">Responsive-ExtendedHand : 投影バーチャルハンドの実物体接触に対して適切な視覚効果を付与するシステム</a></div>
			<!-- <div class='project-title'>投影バーチャルハンドの実物体接触に対して適切な視覚効果を付与するシステムの開発</div> -->
			<div class='project-publication'></div>
			<div class='project-abstract mt-3'>
				<!-- <a routerLink="/projects#eh-phf" routerLinkActive="active"
				  class="project-link">「投影バーチャルハンドインタフェースに対する疑似触覚フィードバックの応用」</a> -->
				「投影バーチャルハンドインタフェースに対する疑似触覚フィードバックの応用」では、物体に触れた際にその物体に適した視覚効果を付与することで、
				疑似触覚フィードバックによりユーザは触れた物体の触感を知覚できることを明らかにしました。
				一方、この手法を実際に利用する場合、事前に手動で物体位置と付与する視覚効果を設定しておく必要がありました。
				そこで本研究では、実環境観測用のRGB-Dカメラと深層学習技術を組み合わせ、
				バーチャルハンドの触れた物体に対して適切な視覚効果をリアルタイムに推定し、バーチャルハンドに付与するシステムを開発しました。
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>


<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/desktophand.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/projects/desktophand" routerLinkActive="active"
				class="project-link">DesktopHand : デスクトップ画面上に表示したバーチャルハンドにより円滑なコミュニケーションを支援するシステム</a></div>
			<!-- <div class='project-title'>DesktopHand : デスクトップ画面上に表示したバーチャルハンドにより円滑なコミュニケーションを支援するシステム</div> -->
			<div class='project-publication'></div>
			<div class='project-abstract mt-3'>
				大画面ディスプレイやスクリーンに資料を表示して行う会議や、コロナ禍より急速に機会が増えたオンライン会議などの際に、
				発表資料に対して聞き手が質問やコメントする際はその内容を口頭でしか伝えられず、
				発表者と聞き手間で意思伝達がスムーズに行えないことが多々あります。
				そこで発表資料が表示されているディスプレイ上に、聞き手が自由に操作できるバーチャルハンドを表示して
				ポインティングやジェスチャ表現を可能にすることで、円滑なコミュニケーションを実現するDesktopHandを提案しました。
				（<a class='project-link' href='https://github.com/youseegreen/DesktopHand/'>アプリケーションはこちら</a>）
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>



<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/hugbot.gif" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/projects/hugbot" routerLinkActive="active"
				class="project-link">Hugbot : ハグインタラクションにおけるロボットの動き方によるコミュニケーション手法の調査</a></div>
			<!-- <div class='project-title'>ハグインタラクションにおけるロボットの動き方によるコミュニケーション手法の調査</div> -->
			<div class='project-publication'>(WISS 2021 Demo)</div>
			<div class='project-abstract mt-3'>
				近年家庭用の癒しロボットの開発や実用化がますます進んできています。
				これらの多くの癒しロボットでは、ユーザがロボットの顔の表情や仕草を見たり（視覚）、ロボットと喋って会話する（聴覚）ことで、
				ユーザ・ロボット間のコミュニケーションを実現します。
				一方、ユーザがロボットをハグする際は、ユーザはロボットの動きを身体で感じる（触覚）ことができますが、
				ロボットの動きに対しユーザがどのような印象を抱くのかはまだ未解明なことが多いです。
				本研究ではハグインタラクション中でも様々な動作が行えるロボットを用いて、ロボットの動作パターンの違いにより、
				ロボットの嬉しいや悲しいといった感情をユーザに伝達できるかを調査します。
			</div>
		</div>
	</div>
</section>

</div>

<div class="mb-5"></div>