<div class="container">
	<div class="row mb-5">
		<div class="col">
			<h3 class="headline">Projects
				<hr />
			</h3>
		</div>
	</div>
</div>

<div class="container">
	<div id="project" class="project-body"></div>

	<section class="card">
		<div class='row'>
			<div class='col-md-5 mb-1'>
				<img src="images/projects/eh_phf.gif" class='project-image'>
			</div>
			<div class='col-md-7 mb-1'>
				<div class='project-title' id="eh-phf">
					<a routerLink="/en/projects/eh_phf" routerLinkActive="active"
					class="project-link">Pseudo-Haptic Feedback for Projected Extended Hand</a></div>
				<div class='project-publication'>(IEEE Access 2020, IEEE World Haptics 2019 Poster, Demo)</div>
				<div class='project-abstract mt-3'>
                    A projected virtual hand interface, synchronized with the user's hand movements, can extend the reachable range of the user's hand in everyday life. However, when the projected virtual hand overlaps with a physical object, the user cannot feel the tactile sensation of the object. In this project, we proposed a method to provide users with the tactile sensation of an object by adding visual effects to the projected virtual hand when it touches the object.
				</div>
			</div>
		</div>
	</section>
<div class="mb-5"></div>

<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/farfeel.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/en/projects/farfeel" routerLinkActive="active"
				  class="project-link">fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic
				  Feedback</a></div>
			<div class='project-publication'>(IEEE VR 2019 Demo)</div>
			<div class='project-abstract mt-3'>
				We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local
				user to feel touching distant objects. The system allows the local and remote users to communicate by
				using the projected virtual hand for the agency of his/her own hands. The necessary haptic
				information is provided to the non-manipulating hand of the local user that does not bother the
				manipulation of the projected virtual hand. We also introduce the possible visual stimulus that could potentially
				provide the sense of the body ownership over the projected virtual hand.
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>


<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/responsive_eh.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/en/projects/responsive_eh" routerLinkActive="active"
				class="project-link">Responsive-ExtendedHand: Adaptive Visuo-Haptic Feedback Recognizing Object Property with RGB-D Camera for Projected Extended Hand</a></div>
			<div class='project-publication'>(IEEE Access 2024)</div>
			<div class='project-abstract mt-3'>
                In the project of "pseudo-haptic feedback for a projected virtual hand interface," we demonstrated that users can perceive the tactile sensation of touched objects by applying appropriate visual effects when the virtual hand touched those objects. However, in practical usage, it is necessary to manually set the positions of objects and the corresponding visual effects beforehand. In this project, we developed a system that combines an RGB-D camera for scene observation with deep learning techniques to estimate suitable visual effects for objects touched by the virtual hand in real-time, thus enabling automatic application of visual effects to the virtual hand.
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>


<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/soundtexture_eh.jpg" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/en/projects/soundtexture_eh" routerLinkActive="active"
				class="project-link">Sound Texture Feedback for Projected Extended Hand</a></div>
			<div class='project-publication'>(IEEE Access 2024)</div>
			<div class='project-abstract mt-3'>
				In this project, we focused on generating tactile sensations from the sounds produced when tracing textured surfaces (such as the rough sound when tracing a bumpy object). In the case of ExtendedHand, users experience a significantly different sensation from physically touching objects when they use the projected extended hand, which amplifies the movement of their hand to reach distant objects that their actual hand cannot reach. In such scenarios, we investigated whether tactile sound feedback should be based on physical phenomena or on different augmented reality principles. 
				<!-- The results indicated that tactile sound pressure should apply the same distance attenuation as physical phenomena. However, when the projected extended hand traced an object at a speed V, it became evident that the feedback sound should be slower than the sound produced when tracing the object manually at a slower pace. -->
			</div>
		</div>
	</div>
</section>
<div class="mb-5"></div>


<section class="card">
	<div class='row'>
		<div class='col-md-5 mb-1'>
			<img src="images/projects/hugbot.gif" class='project-image'>
		</div>
		<div class='col-md-7 mb-1'>
			<div class='project-title'>
				<a routerLink="/en/projects/hugbot" routerLinkActive="active"
				class="project-link">Hugbot: Investigation of Communication Methods in Hug Interactions Based on Robot Movement</a></div>
			<div class='project-publication'>(HRI 2022 LBR)</div>
			<div class='project-abstract mt-3'>
				In recent years, the practical application of home robots have been advancing rapidly. Many of these robots facilitate communication between users and robots through various means, such as users observing the robot's facial expressions and gestures (visual), and engaging in conversation with the robot (auditory). However, when users hug the robot, they can physically feel the robot's movements (tactile), but it is often unclear how users perceive these movements. In this study, we investigated whether different patterns of robot movements during hug interactions can convey emotions such as happiness or sadness to users, using a tensegrity-robot capable of performing various actions during hug interactions.
			</div>
		</div>
	</div>
</section>

</div>

<div class="mb-5"></div>