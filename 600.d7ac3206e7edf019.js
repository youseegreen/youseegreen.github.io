"use strict";(self.webpackChunkyouseegreen_github_io=self.webpackChunkyouseegreen_github_io||[]).push([[600],{7322:(v,d,r)=>{r.r(d),r.d(d,{ProjectsEnModule:()=>x});var l=r(6895),a=r(6429),s=r(5609),t=r(5062);let p=(()=>{class o{constructor(){}ngOnInit(){}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-projects-main-en"]],decls:73,vars:0,consts:[[1,"container"],[1,"row","mb-5"],[1,"col"],[1,"headline"],["id","project",1,"project-body"],[1,"card"],[1,"row"],[1,"col-md-5","mb-1"],["src","images/projects/eh_phf.gif",1,"project-image"],[1,"col-md-7","mb-1"],["id","eh-phf",1,"project-title"],["routerLink","/en/projects/eh_phf","routerLinkActive","active",1,"project-link"],[1,"project-publication"],[1,"project-abstract","mt-3"],[1,"mb-5"],["src","images/projects/farfeel.jpg",1,"project-image"],[1,"project-title"],["routerLink","/en/projects/farfeel","routerLinkActive","active",1,"project-link"],["src","images/projects/responsive_eh.jpg",1,"project-image"],["routerLink","/en/projects/responsive_eh","routerLinkActive","active",1,"project-link"],["src","images/projects/soundtexture_eh.jpg",1,"project-image"],["routerLink","/en/projects/soundtexture_eh","routerLinkActive","active",1,"project-link"],["src","images/projects/hugbot.gif",1,"project-image"],["routerLink","/en/projects/hugbot","routerLinkActive","active",1,"project-link"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"div",2)(3,"h3",3),t._uU(4,"Projects "),t._UZ(5,"hr"),t.qZA()()()(),t.TgZ(6,"div",0),t._UZ(7,"div",4),t.TgZ(8,"section",5)(9,"div",6)(10,"div",7),t._UZ(11,"img",8),t.qZA(),t.TgZ(12,"div",9)(13,"div",10)(14,"a",11),t._uU(15,"Pseudo-Haptic Feedback for Projected Extended Hand"),t.qZA()(),t.TgZ(16,"div",12),t._uU(17,"(IEEE Access 2020, IEEE World Haptics 2019 Poster, Demo)"),t.qZA(),t.TgZ(18,"div",13),t._uU(19," A projected virtual hand interface, synchronized with the user's hand movements, can extend the reachable range of the user's hand in everyday life. However, when the projected virtual hand overlaps with a physical object, the user cannot feel the tactile sensation of the object. In this project, we proposed a method to provide users with the tactile sensation of an object by adding visual effects to the projected virtual hand when it touches the object. "),t.qZA()()()(),t._UZ(20,"div",14),t.TgZ(21,"section",5)(22,"div",6)(23,"div",7),t._UZ(24,"img",15),t.qZA(),t.TgZ(25,"div",9)(26,"div",16)(27,"a",17),t._uU(28,"fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic Feedback"),t.qZA()(),t.TgZ(29,"div",12),t._uU(30,"(IEEE VR 2019 Demo)"),t.qZA(),t.TgZ(31,"div",13),t._uU(32," We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local user to feel touching distant objects. The system allows the local and remote users to communicate by using the projected virtual hand for the agency of his/her own hands. The necessary haptic information is provided to the non-manipulating hand of the local user that does not bother the manipulation of the projected virtual hand. We also introduce the possible visual stimulus that could potentially provide the sense of the body ownership over the projected virtual hand. "),t.qZA()()()(),t._UZ(33,"div",14),t.TgZ(34,"section",5)(35,"div",6)(36,"div",7),t._UZ(37,"img",18),t.qZA(),t.TgZ(38,"div",9)(39,"div",16)(40,"a",19),t._uU(41,"Responsive-ExtendedHand: Adaptive Visuo-Haptic Feedback Recognizing Object Property with RGB-D Camera for Projected Extended Hand"),t.qZA()(),t.TgZ(42,"div",12),t._uU(43,"(IEEE Access 2024)"),t.qZA(),t.TgZ(44,"div",13),t._uU(45,' In the project of "pseudo-haptic feedback for a projected virtual hand interface," we demonstrated that users can perceive the tactile sensation of touched objects by applying appropriate visual effects when the virtual hand touched those objects. However, in practical usage, it is necessary to manually set the positions of objects and the corresponding visual effects beforehand. In this project, we developed a system that combines an RGB-D camera for scene observation with deep learning techniques to estimate suitable visual effects for objects touched by the virtual hand in real-time, thus enabling automatic application of visual effects to the virtual hand. '),t.qZA()()()(),t._UZ(46,"div",14),t.TgZ(47,"section",5)(48,"div",6)(49,"div",7),t._UZ(50,"img",20),t.qZA(),t.TgZ(51,"div",9)(52,"div",16)(53,"a",21),t._uU(54,"Sound Texture Feedback for Projected Extended Hand"),t.qZA()(),t.TgZ(55,"div",12),t._uU(56,"(IEEE Access 2024)"),t.qZA(),t.TgZ(57,"div",13),t._uU(58," In this project, we focused on generating tactile sensations from the sounds produced when tracing textured surfaces (such as the rough sound when tracing a bumpy object). In the case of ExtendedHand, users experience a significantly different sensation from physically touching objects when they use the projected extended hand, which amplifies the movement of their hand to reach distant objects that their actual hand cannot reach. In such scenarios, we investigated whether tactile sound feedback should be based on physical phenomena or on different augmented reality principles. "),t.qZA()()()(),t._UZ(59,"div",14),t.TgZ(60,"section",5)(61,"div",6)(62,"div",7),t._UZ(63,"img",22),t.qZA(),t.TgZ(64,"div",9)(65,"div",16)(66,"a",23),t._uU(67,"Hugbot: Investigation of Communication Methods in Hug Interactions Based on Robot Movement"),t.qZA()(),t.TgZ(68,"div",12),t._uU(69,"(HRI 2022 LBR)"),t.qZA(),t.TgZ(70,"div",13),t._uU(71," In recent years, the practical application of home robots have been advancing rapidly. Many of these robots facilitate communication between users and robots through various means, such as users observing the robot's facial expressions and gestures (visual), and engaging in conversation with the robot (auditory). However, when users hug the robot, they can physically feel the robot's movements (tactile), but it is often unclear how users perceive these movements. In this study, we investigated whether different patterns of robot movements during hug interactions can convey emotions such as happiness or sadness to users, using a tensegrity-robot capable of performing various actions during hug interactions. "),t.qZA()()()()(),t._UZ(72,"div",14))},dependencies:[a.rH,a.Od],styles:[".project-image[_ngcontent-%COMP%]{width:100%}.card[_ngcontent-%COMP%]{padding:8px;background:#fff;border-radius:5px;box-shadow:0 0 3px #ccc}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:22px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:0rem;margin-left:0rem}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;font-family:Roboto;margin-left:1em}.project-publication[_ngcontent-%COMP%], .project-keyword[_ngcontent-%COMP%]{text-align:left;font-size:17px;font-family:Roboto}.project-abstract[_ngcontent-%COMP%]{text-align:left;font-size:16px;font-family:Roboto}.project-link[_ngcontent-%COMP%]{color:#007641;text-decoration:none}.project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}"]})}return o})(),u=(()=>{class o{constructor(){}ngOnInit(){}copy_bibtex(n){const e=document.createElement("textarea");e.textContent="access"==n?"@article{sato2020modifying,\n  title={Modifying texture perception with pseudo-haptic feedback for a projected virtual hand interface},\n  author={Sato, Yushi and Hiraki, Takefumi and Tanabe, Naruki and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke},\n  journal={IEEE Access},\n  volume={8},\n  pages={120473--120488},\n  year={2020},\n  publisher={IEEE}\n}":"@inproceedings{sato2019pseudo,\n  title={Pseudo-haptic feedback in a projected virtual hand for tactile perception of textures},\n  author={Sato, Yushi and Tanabe, Naruki and Morita, Kohei and Hiraki, Takefumi and Punpongsanon, Parinya and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke},\n  booktitle={Proc. IEEE World Haptics Conf.},\n  pages={1--2},\n  year={2019}\n}",document.body.appendChild(e),document.getSelection().selectAllChildren(e),document.execCommand("Copy"),document.body.removeChild(e),alert("\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u306b\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f\uff01\n\n\u30b3\u30d4\u30fc\u3057\u305f\u30c6\u30ad\u30b9\u30c8\uff1a\n"+e.textContent)}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-eh_phf-en"]],decls:45,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title-date"],[1,"project-title"],[1,"project-title-keywords"],[1,"project-title-abstract"],[1,"project-image-layout"],[1,"row"],[1,"col-md-12"],[1,"project-image"],["src","images/projects/eh_phf.gif",2,"width","95%","max-width","560px"],["width","560","height","315","src","https://www.youtube.com/embed/jJuDirkXyRI","title","YouTube video player","frameborder","0","allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture","allowfullscreen","",2,"width","95%","max-width","560px"],[1,"project-body-layout"],[1,"project-section"],[1,"project-sec-title"],[1,"project-body"],["href","https://ieeexplore.ieee.org/document/9130660","target","_new",1,"project-link"],[1,"project-link",3,"click"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h6",2),t._uU(3,"2024-02-29"),t.qZA(),t.TgZ(4,"h1",3),t._uU(5," Pseudo Haptic Feedback for Projected Extended Hand "),t._UZ(6,"hr"),t.qZA(),t.TgZ(7,"h3",4),t._uU(8,"Keywords : projected virtual hand, pseudo-haptic feedback, tactile sensation"),t.qZA(),t.TgZ(9,"h2",5),t._uU(10," A projected virtual hand interface, synchronized with the user's hand movements, can extend the reachable range of the user's hand in everyday life. However, when the projected virtual hand overlaps with a physical object, the user cannot feel the tactile sensation of the object. In this project, we proposed a method to provide users with the tactile sensation of an object by adding visual effects to the projected virtual hand when it touches the object. "),t.qZA(),t.TgZ(11,"div",6)(12,"div",7)(13,"div",8)(14,"div",9),t._UZ(15,"img",10),t.qZA()()()(),t.TgZ(16,"div",6)(17,"div",7)(18,"div",8)(19,"div",9),t._UZ(20,"iframe",11),t.qZA()()()()(),t.TgZ(21,"div",12)(22,"div",13)(23,"h3",14),t._uU(24," Publications "),t._UZ(25,"hr"),t.qZA(),t.TgZ(26,"div",15)(27,"ul")(28,"li"),t._uU(29,'Yushi Sato, Takefumi Hiraki, Naruki Tanabe, Haruka Matsukura, Daisuke Iwai, and Kosuke Sato, "Modifying Texture Perception with Pseudo-Haptic Feedback for a Projected Virtual Hand Interface," '),t.TgZ(30,"i"),t._uU(31,"IEEE Access"),t.qZA(),t._uU(32,", Vol. 8, pp. 120473-120488, 2020.\xa0 "),t.TgZ(33,"a",16),t._uU(34,"[IEEE Eplore]"),t.qZA(),t._uU(35,"\xa0 "),t.TgZ(36,"span",17),t.NdJ("click",function(){return i.copy_bibtex("access")}),t._uU(37,"[Bibtex]"),t.qZA()(),t.TgZ(38,"li"),t._uU(39,'Yushi Sato, Naruki Tanabe, Kohei Morita, Takefumi Hiraki, Parinya Punpongsanon, Haruka Matsukura, Daisuke Iwai, and Kosuke Sato, "Pseudo-Haptic Feedback in a Projected Virtual Hand for Tactile Perception of Textures," In '),t.TgZ(40,"i"),t._uU(41,"Adjunct Proceedings of the 2019 IEEE World Haptics Conference (WHC 2019)"),t.qZA(),t._uU(42,", pp.WP1P.09:1-2, July, 2019.\xa0 "),t.TgZ(43,"span",17),t.NdJ("click",function(){return i.copy_bibtex("whc")}),t._uU(44,"[Bibtex]"),t.qZA()()()()()()())},styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})(),g=(()=>{class o{constructor(){}ngOnInit(){}copy_bibtex(n){const e=document.createElement("textarea");e.textContent="@inproceedings{tanabe2019farfeel,\n  title={FARFEEL: Providing haptic sensation of touched objects using visuo-haptic feedback},\n  author={Tanabe, Naruki and Sato, Yushi and Morita, Kohei and Inagaki, Michiya and Fujino, Yuichi and Punpongsanon, Parinya and Matsukura, Haruka and Iwai, Daisuke and Sato, Kosuke},\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},\n  pages={1355--1356},\n  year={2019},\n  publisher={IEEE}\n}",document.body.appendChild(e),document.getSelection().selectAllChildren(e),document.execCommand("Copy"),document.body.removeChild(e),alert("\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u306b\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f\uff01\n\n\u30b3\u30d4\u30fc\u3057\u305f\u30c6\u30ad\u30b9\u30c8\uff1a\n"+e.textContent)}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-farfeel-en"]],decls:41,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title-date"],[1,"project-title"],[1,"project-title-keywords"],[1,"project-title-abstract"],[1,"project-image-layout"],[1,"row"],[1,"col-md-12"],[1,"project-image"],["src","images/projects/farfeel.jpg",2,"width","95%","max-width","560px"],["width","560","height","315","src","https://www.youtube.com/embed/WekmX3iYCRI","title","YouTube video player","frameborder","0","allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture","allowfullscreen","",2,"width","95%","max-width","560px"],[1,"project-body-layout"],[1,"project-section"],[1,"project-sec-title"],[1,"project-body"],["href","https://ieeexplore.ieee.org/abstract/document/8798195","target","_new",1,"project-link"],[1,"project-link",3,"click"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h6",2),t._uU(3,"2024-02-29"),t.qZA(),t.TgZ(4,"h1",3),t._uU(5," fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic Feedback "),t._UZ(6,"hr"),t.qZA(),t.TgZ(7,"h3",4),t._uU(8,"Keywords : projected virtual hand, teleexistence, telepresence, haptic feedback, pseudo-haptic feedback"),t.qZA(),t.TgZ(9,"h2",5),t._uU(10," We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local user to feel touching distant objects. The system allows the local and remote users to communicate by using the projected virtual hand (VH) for the agency of his/her own hands. The necessary haptic information is provided to the non-manipulating hand of the local user that does not bother the manipulation of the projected VH. We also introduce the possible visual stimulus that could potentially provide the sense of the body ownership over the projected VH. "),t.qZA(),t.TgZ(11,"div",6)(12,"div",7)(13,"div",8)(14,"div",9),t._UZ(15,"img",10),t.qZA()()()(),t.TgZ(16,"div",6)(17,"div",7)(18,"div",8)(19,"div",9),t._UZ(20,"iframe",11),t.qZA()()()()(),t.TgZ(21,"div",12)(22,"div",13)(23,"h3",14),t._uU(24," Publications "),t._UZ(25,"hr"),t.qZA(),t.TgZ(26,"div",15)(27,"ul")(28,"li"),t._uU(29,"Naruki Tanabe, "),t.TgZ(30,"b"),t._uU(31,"Yushi Sato"),t.qZA(),t._uU(32,', Kohei Morita, Michiya Inagaki, Yuichi Fujino, Parinya Punpongsanon, Haruka Matsukura, Daisuke Iwai, and Kosuke Sato, "fARFEEL: Providing Haptic Sensation of Touched Objects using Visuo-Haptic Feedback," In '),t.TgZ(33,"i"),t._uU(34,"Proceedings of the 26th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2019)"),t.qZA(),t._uU(35,", pp.1355-1356, March, 2019.\xa0 "),t.TgZ(36,"a",16),t._uU(37,"[IEEE Eplore]"),t.qZA(),t._uU(38,"\xa0 "),t.TgZ(39,"span",17),t.NdJ("click",function(){return i.copy_bibtex("farfeel")}),t._uU(40,"[Bibtex]"),t.qZA()()()()()()())},styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})(),m=(()=>{class o{constructor(){}ngOnInit(){console.log("responsive-eh-en")}copy_bibtex(n){const e=document.createElement("textarea");e.textContent="@article{sato2024responsive,\n  title={Responsive-ExtendedHand: Adaptive Visuo-Haptic Feedback Recognizing Object Property with RGB-D Camera for Projected Extended Hand},\n  journal={IEEE Access},\n  volume={12},\n  pages={38247--38257},\n  year={2024},\n  publisher={IEEE}\n}",document.body.appendChild(e),document.getSelection().selectAllChildren(e),document.execCommand("Copy"),document.body.removeChild(e),alert("Copied!\n\n"+e.textContent)}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-responsive_eh-en"]],decls:40,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title-date"],[1,"fas","fa-sync-alt"],[1,"project-title"],[1,"project-title-keywords"],[1,"project-title-abstract"],["routerLink","/projects/eh_phf","routerLinkActive","active",1,"project-link"],[1,"project-image-layout"],[1,"row"],[1,"col-md-12"],[1,"project-image"],["src","images/projects/responsive_eh.jpg",2,"width","95%","max-width","560px"],[1,"project-body-layout"],[1,"project-section"],[1,"project-sec-title"],[1,"project-body"],["href","https://ieeexplore.ieee.org/document/","target","_new",1,"project-link"],[1,"project-link",3,"click"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h6",2),t._uU(3,"2024-02-29\u2003"),t._UZ(4,"i",3),t._uU(5," 2024-03-08"),t.qZA(),t.TgZ(6,"h1",4),t._uU(7," Responsive-ExtendedHand: Adaptive Visuo-Haptic Feedback Recognizing Object Property with RGB-D Camera for Projected Extended Hand"),t._UZ(8,"hr"),t.qZA(),t.TgZ(9,"h3",5),t._uU(10,"Keywords : projected virtual hand, pseudo-haptic feedback, deep learning"),t.qZA(),t.TgZ(11,"h2",6),t._uU(12," In the project of "),t.TgZ(13,"a",7),t._uU(14,'"pseudo-haptic feedback for projected extended hand,"'),t.qZA(),t._uU(15," we demonstrated that users can perceive the tactile sensation of touched objects by applying appropriate visual effects when the virtual hand touched those objects. However, in practical usage, it is necessary to manually set the positions of objects and the corresponding visual effects beforehand. In this project, we developed a system that combines an RGB-D camera for scene observation with deep learning techniques to estimate suitable visual effects for objects touched by the virtual hand in real-time, thus enabling automatic application of visual effects to the virtual hand. "),t.qZA(),t.TgZ(16,"div",8)(17,"div",9)(18,"div",10)(19,"div",11),t._UZ(20,"img",12),t.qZA()()()()(),t.TgZ(21,"div",13)(22,"div",14)(23,"h3",15),t._uU(24," Publications "),t._UZ(25,"hr"),t.qZA(),t.TgZ(26,"div",16)(27,"ul")(28,"li")(29,"b"),t._uU(30,"Yushi Sato"),t.qZA(),t._uU(31,', Daisuke Iwai, and Kosuke Sato, "Responsive-ExtendedHand: Adaptive Visuo-Haptic Feedback Recognizing Object Property with RGB-D Camera for Projected Extended Hand," '),t.TgZ(32,"i"),t._uU(33,"IEEE Access"),t.qZA(),t._uU(34,", Vol. 12, pp. 38247-38257, 2024.\xa0 "),t.TgZ(35,"a",17),t._uU(36,"[IEEE Eplore]"),t.qZA(),t._uU(37,"\xa0 "),t.TgZ(38,"span",18),t.NdJ("click",function(){return i.copy_bibtex("responsive_eh")}),t._uU(39,"[Bibtex]"),t.qZA()()()()()()())},dependencies:[a.rH,a.Od],styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})(),h=(()=>{class o{constructor(){}ngOnInit(){}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-desktophand-en"]],decls:4,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h1",2),t._uU(3," TBA "),t.qZA()()())},styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})(),f=(()=>{class o{constructor(){}ngOnInit(){}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-hugbot-en"]],decls:31,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title-date"],[1,"project-title"],[1,"project-title-keywords"],[1,"project-title-abstract"],[1,"project-image-layout"],[1,"row"],[1,"col-md-12"],[1,"project-image"],["src","images/projects/hugbot.gif",2,"width","95%","max-width","560px"],[1,"project-body-layout"],[1,"project-section"],[1,"project-sec-title"],[1,"project-body"]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h6",2),t._uU(3,"2024-02-29"),t.qZA(),t.TgZ(4,"h1",3),t._uU(5," Hugbot: Investigation of Communication Methods in Hug Interactions Based on Robot Movement "),t._UZ(6,"hr"),t.qZA(),t.TgZ(7,"h3",4),t._uU(8,"Keywords : hug interaction, emotion"),t.qZA(),t.TgZ(9,"h2",5),t._uU(10," In recent years, the practical application of home robots have been advancing rapidly. Many of these robots facilitate communication between users and robots through various means, such as users observing the robot's facial expressions and gestures (visual), and engaging in conversation with the robot (auditory). However, when users hug the robot, they can physically feel the robot's movements (tactile), but it is often unclear how users perceive these movements. In this study, we investigated whether different patterns of robot movements during hug interactions can convey emotions such as happiness or sadness to users, using a tensegrity-robot capable of performing various actions during hug interactions. "),t.qZA(),t.TgZ(11,"div",6)(12,"div",7)(13,"div",8)(14,"div",9),t._UZ(15,"img",10),t.qZA()()()()(),t.TgZ(16,"div",11)(17,"div",12)(18,"h3",13),t._uU(19," Publications "),t._UZ(20,"hr"),t.qZA(),t.TgZ(21,"div",14)(22,"ul")(23,"li"),t._uU(24,"Naoya Yoshimura, "),t.TgZ(25,"b"),t._uU(26,"Yushi Sato"),t.qZA(),t._uU(27,', Yuta Kageyama, Jun Murao, Satoshi Yagi, and Parinya Punpongsanon, "Hugmon: Exploration of Affective Movements for Hug Interaction using Tensegrity Robot," '),t.TgZ(28,"i"),t._uU(29,"HRI 2022 (The Late-Breaking Reports)"),t.qZA(),t._uU(30,", LBR-1000, 2022."),t.qZA()()()()()())},styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})();const j=[{path:"",component:s.v,children:[{path:"",component:p,data:{title:"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8 | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"\u4f50\u85e4\u512a\u5fd7\u304c\u95a2\u308f\u3063\u3066\u3044\u308b\u7814\u7a76\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u3059\u3002",keyword:"\u4f50\u85e4\u512a\u5fd7,Yushi Sato,\u5927\u962a\u5927\u5b66,\u57fa\u790e\u5de5\u5b66\u7814\u7a76\u79d1,\u4f50\u85e4\u7814\u7a76\u5ba4,ExtendedHand,\u7591\u4f3c\u89e6\u899a,fARFEEL,desktophand,hagbot",ogUrl:"https://yushisato.com/projects/"}},{path:"eh_phf",component:u,data:{title:"Pseudo-haptic feedback for ExtendedHand | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"\u6295\u5f71\u30d0\u30fc\u30c1\u30e3\u30eb\u30cf\u30f3\u30c9\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306b\u5bfe\u3059\u308b\u7591\u4f3c\u89e6\u899a\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306e\u5fdc\u7528",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,ExtendedHand,projected virtual hand,pseudo-hatpic feedback,tactile sensation",ogUrl:"https://yushisato.com/projects/eh_phf"}},{path:"farfeel",component:g,data:{title:"fARFEEL | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic Feedback",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,fARFEEL,projected virtual hand,teleexistence,telepresence,haptic feedback,pseudo-haptic feedback",ogUrl:"https://yushisato.com/projects/farfeel"}},{path:"responsive_eh",component:m,data:{title:"Responsive-ExtendedHand | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"Responsive-ExtendedHand : \u6295\u5f71\u30d0\u30fc\u30c1\u30e3\u30eb\u30cf\u30f3\u30c9\u306e\u5b9f\u7269\u4f53\u63a5\u89e6\u306b\u5bfe\u3057\u3066\u9069\u5207\u306a\u8996\u899a\u52b9\u679c\u3092\u4ed8\u4e0e\u3059\u308b\u30b7\u30b9\u30c6\u30e0",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,Responsive-ExtendedHand,projected virtual hand,pseudo-hatpic feedback,deep learning",ogUrl:"https://yushisato.com/en/projects/responsive_eh"}},{path:"desktophand",component:h,data:{title:"DesktopHand | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"DesktopHand : \u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u753b\u9762\u4e0a\u306b\u8868\u793a\u3057\u305f\u30d0\u30fc\u30c1\u30e3\u30eb\u30cf\u30f3\u30c9\u306b\u3088\u308a\u5186\u6ed1\u306a\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u652f\u63f4\u3059\u308b\u30b7\u30b9\u30c6\u30e0",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,DesktopHand,virtual Hand",ogUrl:"https://yushisato.com/projects/desktophand"}},{path:"soundtexture_eh",component:(()=>{class o{constructor(){}ngOnInit(){}copy_bibtex(n){const e=document.createElement("textarea");e.textContent="@article{sato2024soundtexture,\n  title={Sound Texture Feedback for a Projected Extended Hand Interface},\n  author={Sato, Yushi and Iwai, Daisuke and Sato, Kosuke},\n  journal={IEEE Access},\n  volume={12},\n  pages={27673--27682},\n  year={2024},\n  publisher={IEEE}\n}",document.body.appendChild(e),document.getSelection().selectAllChildren(e),document.execCommand("Copy"),document.body.removeChild(e),alert("Copied!\n\n"+e.textContent)}static \u0275fac=function(e){return new(e||o)};static \u0275cmp=t.Xpm({type:o,selectors:[["app-soundtexture_eh-en"]],decls:53,vars:0,consts:[[1,"container"],[1,"project-title-layout"],[1,"project-title-date"],[1,"project-title"],[1,"project-title-publication"],[1,"project-title-keywords"],[1,"project-title-abstract"],[1,"project-image-layout"],[1,"row"],[1,"col-md-12"],[1,"project-image"],["src","images/projects/soundtexture_eh.jpg",2,"width","95%","max-width","560px"],[1,"project-body-layout"],[1,"project-section"],[1,"project-sec-title"],[1,"project-body"],["href","https://ieeexplore.ieee.org/document/10440090","target","_new",1,"project-link"],[1,"project-link",3,"click"],[2,"margin-left","1em","margin-top","1em"],[2,"margin-left","1em"],["href","data/soundtexture_carpet.wav","download",""],["href","data/soundtextures_bumpyboard.zip","download",""],["href","data/unit_soundtexture_bumpyboard.wav","download",""]],template:function(e,i){1&e&&(t.TgZ(0,"div",0)(1,"div",1)(2,"h6",2),t._uU(3,"2024-02-29"),t.qZA(),t.TgZ(4,"h1",3),t._uU(5,"Sound Texture Feedback ExtendedHand"),t._UZ(6,"hr"),t.qZA(),t.TgZ(7,"h3",4),t._uU(8,"(IEEE Access 2024)"),t.qZA(),t.TgZ(9,"h3",5),t._uU(10,"Keywords: projected extended hand, cross-modal effect, sound texture, pseudo-haptics"),t.qZA(),t.TgZ(11,"h2",6),t._uU(12,"In this project, we focused on generating tactile sensations from the sounds produced when tracing textured surfaces (such as the rough sound when tracing a bumpy object). In the case of ExtendedHand, users experience a significantly different sensation from physically touching objects when they use the projected extended hand, which amplifies the movement of their hand to reach distant objects that their actual hand cannot reach. In such scenarios, we investigated whether tactile sound feedback should be based on physical phenomena or on different augmented reality principles. "),t.qZA()(),t.TgZ(13,"div",7)(14,"div",8)(15,"div",9)(16,"div",10),t._UZ(17,"img",11),t.qZA()()()(),t.TgZ(18,"div",12)(19,"div",13)(20,"h3",14),t._uU(21," Publications "),t._UZ(22,"hr"),t.qZA(),t.TgZ(23,"div",15)(24,"ul")(25,"li")(26,"b"),t._uU(27,"Yushi Sato"),t.qZA(),t._uU(28,', Daisuke Iwai, and Kosuke Sato, "Sound Texture Feedback for a Projected Extended Hand Interface," '),t.TgZ(29,"i"),t._uU(30,"IEEE Access"),t.qZA(),t._uU(31,", Vol. 12, pp. 27673-27682, February, 2024.\xa0 "),t.TgZ(32,"a",16),t._uU(33,"[IEEE Eplore]"),t.qZA(),t._uU(34,"\xa0 "),t.TgZ(35,"span",17),t.NdJ("click",function(){return i.copy_bibtex("soundtexture-eh")}),t._uU(36,"[Bibtex]"),t.qZA()(),t.TgZ(37,"div",18)(38,"b"),t._uU(39,"Supplemental Material"),t.qZA()(),t.TgZ(40,"ul",19)(41,"li"),t._uU(42,"sound texture of the carpet: "),t.TgZ(43,"a",20),t._uU(44,"soundtexture_carpet.wav"),t.qZA()(),t.TgZ(45,"li"),t._uU(46,"sound textures of the bumpy board: "),t.TgZ(47,"a",21),t._uU(48,"soundtextures_bumpyboard.zip"),t.qZA()(),t.TgZ(49,"li"),t._uU(50,"unit sound texture of the bumpy board: "),t.TgZ(51,"a",22),t._uU(52,"unit sound texture of the bumpy board"),t.qZA()()()()()()()())},styles:[".project-image[_ngcontent-%COMP%]{width:100%}.project-title[_ngcontent-%COMP%]{text-align:left;font-size:23px;font-family:Roboto;font-weight:700;color:#007641;margin-bottom:.5rem;margin-left:.5rem}.project-title-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto;margin-bottom:5rem}.project-title-date[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#aaa;margin-left:.6rem}.project-title-link[_ngcontent-%COMP%]{color:#007641}.project-title-link[_ngcontent-%COMP%]:hover{color:#005621;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-title-publication[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem}.project-title-keywords[_ngcontent-%COMP%]{text-align:left;font-size:16px;color:#444;margin-left:.5rem;margin-bottom:3rem}.project-title-abstract[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-left:.5rem;margin-bottom:.5rem}.project-link[_ngcontent-%COMP%], .project-link[_ngcontent-%COMP%]:link, .project-link[_ngcontent-%COMP%]:visited{color:#058c36;text-decoration:none}.project-link[_ngcontent-%COMP%]:hover{color:#005920;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-layout[_ngcontent-%COMP%]{width:98%;margin-right:auto;margin-left:auto}.project-section[_ngcontent-%COMP%]{margin-bottom:5rem}.project-sec-title[_ngcontent-%COMP%]{text-align:left;font-size:20px;font-weight:700;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-sec-title-link[_ngcontent-%COMP%]{color:#000;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-sec-title-link[_ngcontent-%COMP%]:hover{color:#004611}.project-body[_ngcontent-%COMP%]{text-align:left;font-size:17px;color:#000;margin-bottom:.5rem;margin-left:.5rem}.project-body-link[_ngcontent-%COMP%]{color:#000;text-decoration:underline}.project-body-link[_ngcontent-%COMP%]:hovor{color:#111;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.project-body-strong[_ngcontent-%COMP%]{color:#006441}.project-image-layout[_ngcontent-%COMP%]{margin-top:3rem;margin-bottom:3rem}.project-image[_ngcontent-%COMP%]{width:100%;text-align:center;margin:0 auto}#popup[_ngcontent-%COMP%]{width:30%;line-height:100px;background:#000;padding:0 4%;box-sizing:border-box;display:none;position:fixed;top:50%;left:50%;transform:translate(-50%,-50%)}"]})}return o})(),data:{title:"Sound Texture ExtendedHand | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"TBA",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,ExtendedHand,sound texture,cross-modal effect",ogUrl:"https://yushisato.com/projects/soundtexture_eh"}},{path:"hugbot",component:f,data:{title:"Hugbot | \u4f50\u85e4 \u512a\u5fd7 (Yushi Sato)",description:"Hugbot : \u30cf\u30b0\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u306b\u304a\u3051\u308b\u30ed\u30dc\u30c3\u30c8\u306e\u52d5\u304d\u65b9\u306b\u3088\u308b\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u624b\u6cd5\u306e\u8abf\u67fb",keyword:"\u4f50\u85e4\u512a\u5fd7,youseegreen,Yushi Sato,Hugbot,hug interaction,emotion",ogUrl:"https://yushisato.com/projects/hugbot"}}]}];let _=(()=>{class o{static \u0275fac=function(e){return new(e||o)};static \u0275mod=t.oAB({type:o});static \u0275inj=t.cJS({imports:[a.Bz.forChild(j),a.Bz]})}return o})(),x=(()=>{class o{static \u0275fac=function(e){return new(e||o)};static \u0275mod=t.oAB({type:o});static \u0275inj=t.cJS({imports:[l.ez,_]})}return o})()}}]);